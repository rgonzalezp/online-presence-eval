Project: AI Model Evaluation Framework

Overview:
This project is a framework for evaluating and comparing different AI language models' responses to various prompts, with a focus on researcher identity verification and web search capabilities.

Core Features:
1. Model Integration
   - Supports multiple AI providers:
     * OpenAI (GPT models)
     * Anthropic (Claude)
     * Google (Gemini)
     * X.AI (Grok)
     * Perplexity (Sonar) - Web-search only mode
   - Each model implementation includes:
     * Web search capability when available
     * Metadata extraction
     * Error handling
     * Response formatting
     * Full API response capture
   - Multi-turn conversation support:
     * Message history tracking
     * Context preservation
     * Flexible input formats (string or message list)
     * Automatic history management

2. Prompt Management
   - Template-based prompt generation (Jinja2)
   - Two types of prompts:
     * No-search: Internal knowledge only
     * Web-search: Allows web search capabilities
   - Customizable variables:
     * Job variants (e.g., "accessibility researcher", "AI auditing researcher")
     * Name variants (e.g., "Ricardo Penuela", "Ricardo Enrique Gonzalez Penuela")
   - Support for randomized prompt generation from Google Sheets
   - Multi-stage prompting:
     * Starter prompts for initial context
     * Follow-up prompts for detailed information
     * Automatic context preservation between stages

3. Flexible Execution
   - Interactive model selection:
     * By number (e.g., "123")
     * By provider (e.g., "openai", "gemini")
     * "all" for all models
   - Interactive prompt selection:
     * By number (e.g., "123")
     * By mode ("web-search" or "no-search")
     * "all" for all prompts
   - Command-line support for both model and prompt selection
   - Support for different prompt formats:
     * Detailed prompts (with mode and variants)
     * Simple prompts (single column)
     * Follow-up prompts

4. Results Management
   - CSV output with comprehensive data:
     * Model responses
     * Metadata
     * API responses
     * Prompt variants
     * Job and name variants
   - Results include:
     * Full response text
     * Model-specific metadata
     * Search results (when applicable)
     * Error information (if any)
   - Detailed API logging:
     * JSON-based logging of all API calls
     * Timestamps for each interaction
     * Complete request and response data
     * Automatic serialization of complex objects
     * Persistent storage in api_calls.json
     * Conversation history tracking
     * Refusal detection results

5. Response Analysis
   - Integrated refusal detection:
     * Uses Minos-v1 model for classification
     * Binary classification (Refusal/Non-refusal)
     * Confidence scores for each prediction
     * Real-time analysis during model execution
   - Multi-turn analysis:
     * Context-aware refusal detection
     * History-based response evaluation
     * Automatic filtering of refusing models

6. Security and Configuration
   - Centralized API key management:
     * Separate configuration for API keys
     * Template-based setup
     * Git-ignored sensitive data
   - Model configuration:
     * Provider-specific settings
     * Model version management
     * Clean separation of concerns

Configuration:
- Models configured in YAML files:
  * paid_models.yml: Production models configuration
  * models.yml: Additional model configurations
  * api_keys.yml: API keys (git-ignored)
- Prompt templates in prompts/ directory
- CSV files for prompt permutations and results
- API logs stored in utils/api_calls.json

Usage:
1. Basic execution:
   python run_models.py

2. Specific model selection:
   python run_models.py 12 web-search

3. All models with specific prompts:
   python run_models.py all 123

4. Provider-specific:
   python run_models.py gemini web-search

Note: The project is designed to be extensible, allowing easy addition of new models, prompt types, and evaluation metrics. 